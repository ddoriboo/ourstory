/* tslint:disable */
/**
 * @license
 * SPDX-License-Identifier: Apache-2.0
 */

import {GoogleGenAI, LiveServerMessage, Modality, Session} from '@google/genai';
import {LitElement, css, html} from 'lit';
import {customElement, state} from 'lit/decorators.js';
import {createBlob, decode, decodeAudioData} from './utils';
import './visual-3d';
import {interviewConfig} from './interviewConfig';

@customElement('gdm-live-audio')
export class GdmLiveAudio extends LitElement {
  @state() isRecording = false;
  @state() status = '';
  @state() error = '';
  @state() currentSessionId = 1;
  @state() currentQuestionIndex = 0;
  @state() conversationHistory: {speaker: 'ai' | 'user', text: string, timestamp: Date}[] = [];
  @state() isSessionConnected = false;

  private client: GoogleGenAI;
  private session: Session | null = null;
  private inputAudioContext = new (window.AudioContext ||
    window.webkitAudioContext)({
      sampleRate: 16000,
      latencyHint: 'interactive'
    });
  private outputAudioContext = new (window.AudioContext ||
    window.webkitAudioContext)({
      sampleRate: 24000,
      latencyHint: 'interactive'
    });
  @state() inputNode = this.inputAudioContext.createGain();
  @state() outputNode = this.outputAudioContext.createGain();
  private nextStartTime = 0;
  private mediaStream: MediaStream;
  private sourceNode: AudioBufferSourceNode;
  private scriptProcessorNode: ScriptProcessorNode;
  private sources = new Set<AudioBufferSourceNode>();
  private speechRecognition: any;
  private isTranscribing = false;

  static styles = css`
    #status {
      position: absolute;
      bottom: 5vh;
      left: 0;
      right: 0;
      z-index: 10;
      text-align: center;
    }

    .session-info {
      position: absolute;
      top: 20px;
      left: 20px;
      z-index: 10;
      color: white;
      background: rgba(0, 0, 0, 0.5);
      padding: 20px;
      border-radius: 10px;
      max-width: 400px;
    }

    .session-info h3 {
      margin: 0 0 10px 0;
      font-size: 18px;
    }

    .session-info p {
      margin: 0;
      font-size: 14px;
      opacity: 0.8;
    }

    .session-controls {
      position: absolute;
      top: 20px;
      right: 20px;
      z-index: 10;
      display: flex;
      gap: 10px;
    }

    .session-controls button {
      padding: 8px 16px;
      border: 1px solid rgba(255, 255, 255, 0.3);
      background: rgba(255, 255, 255, 0.1);
      color: white;
      border-radius: 6px;
      cursor: pointer;
      font-size: 14px;
    }

    .session-controls button:hover {
      background: rgba(255, 255, 255, 0.2);
    }

    .session-controls button:disabled {
      opacity: 0.5;
      cursor: not-allowed;
    }

    .conversation-panel {
      position: fixed;
      left: 20px;
      bottom: 20vh;
      top: 120px;
      width: 400px;
      background: rgba(0, 0, 0, 0.8);
      color: white;
      border-radius: 10px;
      padding: 20px;
      overflow-y: auto;
      z-index: 5;
      backdrop-filter: blur(10px);
      border: 1px solid rgba(255, 255, 255, 0.1);
    }

    .conversation-panel h4 {
      margin: 0 0 15px 0;
      color: #4CAF50;
      font-size: 16px;
      border-bottom: 1px solid rgba(255, 255, 255, 0.2);
      padding-bottom: 10px;
    }

    .message {
      margin-bottom: 15px;
      padding: 10px;
      border-radius: 8px;
      line-height: 1.4;
      font-size: 14px;
    }

    .message.ai {
      background: rgba(76, 175, 80, 0.2);
      border-left: 3px solid #4CAF50;
    }

    .message.user {
      background: rgba(33, 150, 243, 0.2);
      border-left: 3px solid #2196F3;
    }

    .message-header {
      font-size: 12px;
      opacity: 0.7;
      margin-bottom: 5px;
      font-weight: bold;
    }

    .message-text {
      white-space: pre-wrap;
      word-wrap: break-word;
    }

    .timestamp {
      font-size: 10px;
      opacity: 0.5;
      margin-top: 5px;
    }

    .controls {
      z-index: 10;
      position: absolute;
      bottom: 10vh;
      left: 0;
      right: 0;
      display: flex;
      align-items: center;
      justify-content: center;
      flex-direction: column;
      gap: 10px;

      button {
        outline: none;
        border: 1px solid rgba(255, 255, 255, 0.2);
        color: white;
        border-radius: 12px;
        background: rgba(255, 255, 255, 0.1);
        width: 64px;
        height: 64px;
        cursor: pointer;
        font-size: 24px;
        padding: 0;
        margin: 0;

        &:hover {
          background: rgba(255, 255, 255, 0.2);
        }
      }

      button[disabled] {
        display: none;
      }
    }
  `;

  constructor() {
    super();
    this.initClient();
    this.initSpeechRecognition();
  }

  private initAudio() {
    this.nextStartTime = this.outputAudioContext.currentTime;
  }

  private initSpeechRecognition() {
    // Web Speech API ì§€ì› í™•ì¸
    const SpeechRecognition = (window as any).SpeechRecognition || (window as any).webkitSpeechRecognition;
    
    if (SpeechRecognition) {
      this.speechRecognition = new SpeechRecognition();
      this.speechRecognition.continuous = true;
      this.speechRecognition.interimResults = false;
      this.speechRecognition.lang = 'ko-KR';
      
      this.speechRecognition.onresult = (event: any) => {
        for (let i = event.resultIndex; i < event.results.length; i++) {
          if (event.results[i].isFinal) {
            const transcript = event.results[i][0].transcript;
            console.log('ì‚¬ìš©ì ìŒì„± ì¸ì‹:', transcript);
            this.addToConversation('user', transcript);
          }
        }
      };
      
      this.speechRecognition.onerror = (event: any) => {
        console.error('Speech recognition error:', event.error);
      };
      
      this.speechRecognition.onend = () => {
        this.isTranscribing = false;
      };
    } else {
      console.warn('Web Speech API not supported');
    }
  }

  private async initClient() {
    this.initAudio();

    // ë‹¤ì–‘í•œ ë°©ë²•ìœ¼ë¡œ API í‚¤ í™•ì¸
    const apiKey = process.env.GEMINI_API_KEY || 
                  process.env.VITE_GEMINI_API_KEY || 
                  (import.meta as any)?.env?.VITE_GEMINI_API_KEY ||
                  'AIzaSyBn158ydMQWCNHWxy2HSPHDZC3Snms2n0w'; // ìƒˆë¡œìš´ API í‚¤ë¡œ í´ë°±
    
    console.log('í™˜ê²½ ë³€ìˆ˜ë“¤:', {
      'process.env.GEMINI_API_KEY': process.env.GEMINI_API_KEY ? 'defined' : 'undefined',
      'process.env.VITE_GEMINI_API_KEY': process.env.VITE_GEMINI_API_KEY ? 'defined' : 'undefined',
      'import.meta.env.VITE_GEMINI_API_KEY': (import.meta as any)?.env?.VITE_GEMINI_API_KEY ? 'defined' : 'undefined',
      'final apiKey': apiKey ? `${apiKey.substring(0, 10)}...` : 'undefined'
    });
    
    if (!apiKey || apiKey === 'PLACEHOLDER_API_KEY') {
      this.updateError('API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. í™˜ê²½ ë³€ìˆ˜ë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”.');
      return;
    }

    this.client = new GoogleGenAI({
      apiKey: apiKey,
    });

    this.outputNode.connect(this.outputAudioContext.destination);

    this.initSession();
  }

  private async initSession() {
    const model = 'gemini-2.5-flash-preview-native-audio-dialog';
    
    // ì´ˆê¸°í™” ì‹œ ì—°ê²° ìƒíƒœë¥¼ falseë¡œ ì„¤ì •
    this.isSessionConnected = false;

    try {
      this.session = await this.client.live.connect({
        model: model,
        callbacks: {
          onopen: () => {
            this.isSessionConnected = true;
            this.updateStatus('ğŸ¤ ë…¹ìŒ ë²„íŠ¼ì„ ëˆŒëŸ¬ ëŒ€í™”ë¥¼ ì‹œì‘í•˜ì„¸ìš”.');
          },
          onmessage: async (message: LiveServerMessage) => {
            // í…ìŠ¤íŠ¸ ì‘ë‹µ ì²˜ë¦¬
            const textPart = message.serverContent?.modelTurn?.parts?.find(
              part => part.text && part.text.trim()
            );
            if (textPart?.text) {
              this.addToConversation('ai', textPart.text);
            }

            // ì˜¤ë””ì˜¤ ì‘ë‹µ ì²˜ë¦¬
            const audio =
              message.serverContent?.modelTurn?.parts[0]?.inlineData;

            if (audio) {
              try {
                // ë” ë¶€ë“œëŸ¬ìš´ ì˜¤ë””ì˜¤ ì¬ìƒì„ ìœ„í•œ ì‹œê°„ ê³„ì‚°
                this.nextStartTime = Math.max(
                  this.nextStartTime,
                  this.outputAudioContext.currentTime + 0.01, // ì•½ê°„ì˜ ë²„í¼ ì‹œê°„ ì¶”ê°€
                );

                const audioBuffer = await decodeAudioData(
                  decode(audio.data),
                  this.outputAudioContext,
                  24000,
                  1,
                );
                
                const source = this.outputAudioContext.createBufferSource();
                source.buffer = audioBuffer;
                
                // ë” ë‚˜ì€ ì˜¤ë””ì˜¤ í’ˆì§ˆì„ ìœ„í•œ gain ì¡°ì •
                const gainNode = this.outputAudioContext.createGain();
                gainNode.gain.value = 1.0;
                source.connect(gainNode);
                gainNode.connect(this.outputNode);
                
                source.addEventListener('ended', () => {
                  this.sources.delete(source);
                });

                source.start(this.nextStartTime);
                this.nextStartTime = this.nextStartTime + audioBuffer.duration;
                this.sources.add(source);
              } catch (audioError) {
                console.error('Audio processing error:', audioError);
              }
            }

            const interrupted = message.serverContent?.interrupted;
            if(interrupted) {
              for(const source of this.sources.values()) {
                source.stop();
                this.sources.delete(source);
              }
              this.nextStartTime = 0;
            }
          },
          onerror: (e: ErrorEvent) => {
            this.isSessionConnected = false;
            this.updateError(`ì—°ê²° ì˜¤ë¥˜: ${e.message}`);
            console.error('Session error:', e);
          },
          onclose: (e: CloseEvent) => {
            this.isSessionConnected = false;
            this.updateStatus(`ì—°ê²°ì´ ì¢…ë£Œë˜ì—ˆìŠµë‹ˆë‹¤: ${e.reason}`);
            console.log('Session closed:', e.reason);
          },
        },
        config: {
          responseModalities: [Modality.AUDIO, Modality.TEXT],
          systemInstruction: interviewConfig.systemInstruction + this.getCurrentSessionPrompt(),
          speechConfig: {
            voiceConfig: {prebuiltVoiceConfig: {voiceName: 'Leda'}},
            languageCode: 'ko-KR'
          },
        },
      });
    } catch (e) {
      this.isSessionConnected = false;
      this.session = null;
      console.error('ì„¸ì…˜ ì´ˆê¸°í™” ì‹¤íŒ¨:', e);
      this.updateError(`ì„¸ì…˜ ì—°ê²°ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤: ${e.message}`);
    }
  }

  private updateStatus(msg: string) {
    this.status = msg;
  }

  private updateError(msg: string) {
    this.error = msg;
  }

  private async startRecording() {
    if (this.isRecording) {
      return;
    }

    this.inputAudioContext.resume();

    this.updateStatus('Requesting microphone access...');

    try {
      this.mediaStream = await navigator.mediaDevices.getUserMedia({
        audio: true,
        video: false,
      });

      this.updateStatus('Microphone access granted. Starting capture...');

      this.sourceNode = this.inputAudioContext.createMediaStreamSource(
        this.mediaStream,
      );
      this.sourceNode.connect(this.inputNode);

      const bufferSize = 512; // ë” í° ë²„í¼ í¬ê¸°ë¡œ ì•ˆì •ì„± í–¥ìƒ
      this.scriptProcessorNode = this.inputAudioContext.createScriptProcessor(
        bufferSize,
        1,
        1,
      );

      this.scriptProcessorNode.onaudioprocess = (audioProcessingEvent) => {
        if (!this.isRecording) return;
        
        // ì„¸ì…˜ ìœ íš¨ì„± ê²€ì‚¬
        if (!this.isSessionValid()) {
          console.warn('ì„¸ì…˜ì´ ìœ íš¨í•˜ì§€ ì•Šì•„ ì˜¤ë””ì˜¤ ì „ì†¡ì„ ì¤‘ë‹¨í•©ë‹ˆë‹¤.');
          this.stopRecording();
          return;
        }

        const inputBuffer = audioProcessingEvent.inputBuffer;
        const pcmData = inputBuffer.getChannelData(0);

        try {
          this.session.sendRealtimeInput({media: createBlob(pcmData)});
        } catch (error) {
          console.error('ì˜¤ë””ì˜¤ ì „ì†¡ ì˜¤ë¥˜:', error);
          this.stopRecording();
          this.updateError('ì˜¤ë””ì˜¤ ì „ì†¡ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.');
        }
      };

      this.sourceNode.connect(this.scriptProcessorNode);
      this.scriptProcessorNode.connect(this.inputAudioContext.destination);

      this.isRecording = true;
      this.updateStatus('ğŸ”´ Recording... Capturing PCM chunks.');
      
      // ìŒì„± ì¸ì‹ ì‹œì‘
      if (this.speechRecognition && !this.isTranscribing) {
        try {
          this.speechRecognition.start();
          this.isTranscribing = true;
        } catch (e) {
          console.error('Speech recognition start error:', e);
        }
      }
    } catch (err) {
      console.error('Error starting recording:', err);
      this.updateStatus(`Error: ${err.message}`);
      this.stopRecording();
    }
  }

  private getCurrentSessionPrompt() {
    const currentSession = interviewConfig.sessions[this.currentSessionId];
    if (!currentSession) return '';
    
    return `\n\n### í˜„ì¬ ì„¸ì…˜: ${currentSession.title}\n\n` +
           `í˜„ì¬ ì§„í–‰ ì¤‘ì¸ ì„¸ì…˜ì˜ ì£¼ìš” ì§ˆë¬¸ë“¤:\n` +
           currentSession.questions.map((q, i) => `${i + 1}. ${q}`).join('\n') +
           `\n\n**ì¤‘ìš”:** ì„¸ì…˜ì´ ì‹œì‘ë˜ë©´ ì¦‰ì‹œ ë‹¤ìŒê³¼ ê°™ì´ ì¸ì‚¬í•´ì£¼ì„¸ìš”: "ì•ˆë…•í•˜ì„¸ìš”, ì–´ë¥´ì‹ ì˜ ì†Œì¤‘í•œ ì¸ìƒ ì´ì•¼ê¸°ë¥¼ ê·€ë‹´ì•„ë“£ê³  ì•„ë¦„ë‹¤ìš´ ìì„œì „ìœ¼ë¡œ ê¸°ë¡í•´ ë“œë¦´ 'ê¸°ì–µì˜ ì•ˆë‚´ì'ì…ë‹ˆë‹¤. ì œê°€ ê³ì—ì„œ ê¸¸ì¡ì´ê°€ ë˜ì–´ë“œë¦´ í…Œë‹ˆ, ê·¸ì € ì˜¤ëœ ì¹œêµ¬ì—ê²Œ ì´ì•¼ê¸°í•˜ë“¯ í¸ì•ˆí•œ ë§ˆìŒìœ¼ë¡œ í•¨ê»˜í•´ ì£¼ì‹œë©´ ë©ë‹ˆë‹¤. ì˜¤ëŠ˜ì€ '${currentSession.title}'ì— ëŒ€í•´ ì´ì•¼ê¸°ë¥¼ ë‚˜ëˆ ë³´ê³ ì í•©ë‹ˆë‹¤. ì¤€ë¹„ë˜ì…¨ì„ ë•Œ í¸í•˜ê²Œ ë§ì”€í•´ì£¼ì„¸ìš”." ê·¸ë¦¬ê³  ì²« ë²ˆì§¸ ì§ˆë¬¸ë¶€í„° ì‹œì‘í•´ì£¼ì„¸ìš”.`;
  }



  private isSessionValid(): boolean {
    return this.session !== null && this.isSessionConnected;
  }

  private addToConversation(speaker: 'ai' | 'user', text: string) {
    this.conversationHistory = [
      ...this.conversationHistory,
      {
        speaker,
        text: text.trim(),
        timestamp: new Date()
      }
    ];
    
    // ëŒ€í™”ê°€ ì¶”ê°€ëœ í›„ ìŠ¤í¬ë¡¤ì„ ë§¨ ì•„ë˜ë¡œ
    this.updateComplete.then(() => {
      const conversationPanel = this.shadowRoot?.querySelector('.conversation-panel');
      if (conversationPanel) {
        conversationPanel.scrollTop = conversationPanel.scrollHeight;
      }
    });
  }

  private stopRecording() {
    if (!this.isRecording && !this.mediaStream && !this.inputAudioContext)
      return;

    this.updateStatus('Stopping recording...');

    this.isRecording = false;

    if (this.scriptProcessorNode && this.sourceNode && this.inputAudioContext) {
      this.scriptProcessorNode.disconnect();
      this.sourceNode.disconnect();
    }

    this.scriptProcessorNode = null;
    this.sourceNode = null;

    if (this.mediaStream) {
      this.mediaStream.getTracks().forEach((track) => track.stop());
      this.mediaStream = null;
    }

    this.updateStatus('Recording stopped. Click Start to begin again.');
    
    // ìŒì„± ì¸ì‹ ì¤‘ì§€
    if (this.speechRecognition && this.isTranscribing) {
      try {
        this.speechRecognition.stop();
        this.isTranscribing = false;
      } catch (e) {
        console.error('Speech recognition stop error:', e);
      }
    }
  }

  private async reset() {
    try {
      // ë¨¼ì € ë…¹ìŒ ì¤‘ì§€
      if (this.isRecording) {
        this.stopRecording();
      }
      
      // ì„¸ì…˜ ë‹«ê¸°
      if (this.session) {
        this.isSessionConnected = false;
        this.session.close();
        this.session = null;
      }
      
      this.conversationHistory = []; // ëŒ€í™” ê¸°ë¡ ì´ˆê¸°í™”
      this.updateStatus('ì„¸ì…˜ì„ ì¬ì‹œì‘í•˜ëŠ” ì¤‘...');
      
      // ì ì‹œ ëŒ€ê¸° í›„ ì¬ì—°ê²°
      await new Promise(resolve => setTimeout(resolve, 1000));
      await this.initSession();
      
      this.updateStatus('ì„¸ì…˜ì´ ì¬ì‹œì‘ë˜ì—ˆìŠµë‹ˆë‹¤.');
    } catch (e) {
      console.error('Reset error:', e);
      this.updateError(`ì„¸ì…˜ ì¬ì‹œì‘ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤: ${e.message}`);
    }
  }

  private async nextSession() {
    if (this.currentSessionId < 12) {
      this.currentSessionId++;
      this.currentQuestionIndex = 0;
      this.updateStatus(`ì„¸ì…˜ ${this.currentSessionId}ë¡œ ì´ë™ ì¤‘...`);
      await this.reset();
      this.updateStatus(`ì„¸ì…˜ ${this.currentSessionId}: ${interviewConfig.sessions[this.currentSessionId]?.title || ''}`);
    }
  }

  private async previousSession() {
    if (this.currentSessionId > 1) {
      this.currentSessionId--;
      this.currentQuestionIndex = 0;
      this.updateStatus(`ì„¸ì…˜ ${this.currentSessionId}ë¡œ ì´ë™ ì¤‘...`);
      await this.reset();
      this.updateStatus(`ì„¸ì…˜ ${this.currentSessionId}: ${interviewConfig.sessions[this.currentSessionId]?.title || ''}`);
    }
  }

  render() {
    const currentSession = interviewConfig.sessions[this.currentSessionId];
    
    return html`
      <div>
        <div class="session-info">
          <h3>ì„¸ì…˜ ${this.currentSessionId}: ${currentSession?.title || ''}</h3>
          <p>ì§ˆë¬¸ ${this.currentQuestionIndex + 1} / ${currentSession?.questions.length || 0}</p>
          <p style="font-size: 12px; color: ${this.isSessionConnected ? '#4CAF50' : '#f44336'};">
            ${this.isSessionConnected ? 'ğŸŸ¢ ì—°ê²°ë¨' : 'ğŸ”´ ì—°ê²° ëŠì–´ì§'}
          </p>
        </div>
        
        <div class="session-controls">
          <button @click=${this.previousSession} ?disabled=${this.currentSessionId <= 1}>
            ì´ì „ ì„¸ì…˜
          </button>
          <button @click=${this.nextSession} ?disabled=${this.currentSessionId >= 12}>
            ë‹¤ìŒ ì„¸ì…˜
          </button>
        </div>
        
        <div class="conversation-panel">
          <h4>ğŸ—£ï¸ ëŒ€í™” ê¸°ë¡</h4>
          ${this.conversationHistory.map(message => html`
            <div class="message ${message.speaker}">
              <div class="message-header">
                ${message.speaker === 'ai' ? 'ğŸ¤– ê¸°ì–µì˜ ì•ˆë‚´ì' : 'ğŸ‘¤ ì–´ë¥´ì‹ '}
              </div>
              <div class="message-text">${message.text}</div>
              <div class="timestamp">
                ${message.timestamp.toLocaleTimeString('ko-KR')}
              </div>
            </div>
          `)}
        </div>
        
        <div class="controls">
          <button
            id="resetButton"
            @click=${this.reset}
            ?disabled=${this.isRecording}>
            <svg
              xmlns="http://www.w3.org/2000/svg"
              height="40px"
              viewBox="0 -960 960 960"
              width="40px"
              fill="#ffffff">
              <path
                d="M480-160q-134 0-227-93t-93-227q0-134 93-227t227-93q69 0 132 28.5T720-690v-110h80v280H520v-80h168q-32-56-87.5-88T480-720q-100 0-170 70t-70 170q0 100 70 170t170 70q77 0 139-44t87-116h84q-28 106-114 173t-196 67Z" />
            </svg>
          </button>
          <button
            id="startButton"
            @click=${this.startRecording}
            ?disabled=${this.isRecording || !this.isSessionConnected}>
            <svg
              viewBox="0 0 100 100"
              width="32px"
              height="32px"
              fill="#c80000"
              xmlns="http://www.w3.org/2000/svg">
              <circle cx="50" cy="50" r="50" />
            </svg>
          </button>
          <button
            id="stopButton"
            @click=${this.stopRecording}
            ?disabled=${!this.isRecording}>
            <svg
              viewBox="0 0 100 100"
              width="32px"
              height="32px"
              fill="#000000"
              xmlns="http://www.w3.org/2000/svg">
              <rect x="0" y="0" width="100" height="100" rx="15" />
            </svg>
          </button>
        </div>

        <div id="status"> ${this.error || this.status} </div>
        <gdm-live-audio-visuals-3d
          .inputNode=${this.inputNode}
          .outputNode=${this.outputNode}></gdm-live-audio-visuals-3d>
      </div>
    `;
  }
}
